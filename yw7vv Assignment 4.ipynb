{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yw7vv Assignment 4.ipynb","version":"0.3.2","provenance":[{"file_id":"18hFyL-FJrsJBxZ4WfwRAY5lCwIWHt8uu","timestamp":1555392454575},{"file_id":"1hQZ4t2l5aFDO0sEs213HsV547c_tH6TL","timestamp":1554445243544}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"-40VPC7MAGGB","colab_type":"text"},"cell_type":"markdown","source":["# Assignment 4: Benchmarking Fashion-MNIST with Deep Neural Nets"]},{"metadata":{"id":"piFzh10hAGGE","colab_type":"text"},"cell_type":"markdown","source":["### CS 4501 Machine Learning - Department of Computer Science - University of Virginia\n","\"The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" - **Zalando Research, Github Repo.**\"\n","\n","Fashion-MNIST is a dataset from the Zalando's article. Each example is a 28x28 grayscale image, associated with a label from 10 classes. They intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n","\n","![Here's an example how the data looks (each class takes three-rows):](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n","\n","In this assignment, you will attempt to benchmark the Fashion-MNIST using Neural Networks. You must use it to train some neural networks on TensorFlow and predict the final output of 10 classes. For deliverables, you must write code in Python and submit this Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.\n"]},{"metadata":{"id":"469YvvIzAGGJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# You might want to use the following packages\n","import numpy as np\n","import os\n","import tensorflow as tf\n","tf.logging.set_verbosity(tf.logging.ERROR) #reduce annoying warning messages\n","from functools import partial\n","\n","# to make this notebook's output stable across runs\n","def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t-PtpH4xAGGG","colab_type":"text"},"cell_type":"markdown","source":["---\n","## 1. PRE-PROCESSING THE DATA (10 pts)\n","\n","You can load the Fashion MNIST directly from Tensorflow. **Partition of the dataset** so that you will have 50,000 examples for training, 10,000 examples for validation, and 10,000 examples for testing. Also, make sure that you platten out each of examples so that it contains only a 1-D feature vector.\n","\n","Write some code to output the dimensionalities of each partition (train, validation, and test sets).\n","\n"]},{"metadata":{"id":"Z2-Ilkesfm7Z","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your code goes here for this section.\n","fmnist = tf.keras.datasets.fashion_mnist.load_data();\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dsiP7SA1rDbg","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","(X_train, y_train), (X_test, y_test)=fmnist\n","X_train, X_valid, y_train, y_valid=train_test_split( X_train, y_train, test_size=1/6, random_state=42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7L1zx6egt8AP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"b62e385c-1bf4-4898-d2c6-7937d64967f4","executionInfo":{"status":"ok","timestamp":1555607676960,"user_tz":240,"elapsed":766,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["\n","print(\"The shape of train_images is \",X_train.shape)\n","print(\"The shape of train_labels is \",y_train.shape)\n","print(\"The shape of test_images is \",X_test.shape)\n","print(\"The shape of test_labels is \",y_test.shape)\n","print(\"The shape of validation_images is \",X_valid.shape)\n","print(\"The shape of validation_labels is \",y_valid.shape)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["The shape of train_images is  (50000, 28, 28)\n","The shape of train_labels is  (50000,)\n","The shape of test_images is  (10000, 28, 28)\n","The shape of test_labels is  (10000,)\n","The shape of validation_images is  (10000, 28, 28)\n","The shape of validation_labels is  (10000,)\n"],"name":"stdout"}]},{"metadata":{"id":"_xyHyxSrNJXL","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n","X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n","X_valid = X_valid.astype(np.float32).reshape(-1, 28*28) / 255.0\n","y_train = y_train.astype(np.int32)\n","y_test = y_test.astype(np.int32)\n","y_valid = y_valid.astype(np.int32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hvsVHxMcOZtv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"89fe5a67-9308-4071-e19c-9bc838859c4a","executionInfo":{"status":"ok","timestamp":1555607681195,"user_tz":240,"elapsed":585,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","image=X_train[55,:].reshape(28,28)\n","plt.imshow(image)\n","plt.show()\n"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntVIPAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Na4CpxLBAGGP","colab_type":"text"},"cell_type":"markdown","source":["- - -\n","## 2. CONSTRUCTION PHASE (30 pts)\n","\n","In this section, define at least three neural networks with different structures. Make sure that the input layer has the right number of inputs. The best structure often is found through a process of trial and error experimentation:\n","- You may start with a fully connected network structure with two hidden layers.\n","- You may try a few settings of the number of nodes in each layer.\n","- You may try a few activation functions to see if they affect the performance.\n","\n","**Important Implementation Note:** For the purpose of learning Tensorflow, you must use low-level TensorFlow API to construct the network. Usage of high-level tools (ie. Keras) is not permited. "]},{"metadata":{"id":"bIJrHPVlAGGQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your code goes here\n","reset_graph()\n","\n","# Set some configuration here\n","n_inputs = 28*28  # Fashion-MNIST\n","learning_rate = 0.01\n","n_outputs = 10\n","\n","\n","# Construct placeholder for the input layer\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","#add n_outputs by myself\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ekhXQnSpPWT_","colab_type":"code","colab":{}},"cell_type":"code","source":["def leaky_relu(z, name=None):\n","    return tf.maximum(0.01 * z, z, name=name)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iDrFp7KKils6","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"dnn1\"):\n","  #implementation of the first net here\n","#     dnn1_hidden1 = 500\n","#     dnn1_hidden2 = 250\n","    dnn1_hidden1 = 400\n","    dnn1_hidden2 = 200\n","    hidden1 = tf.layers.dense(X, dnn1_hidden1, activation=leaky_relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, dnn1_hidden2, activation=leaky_relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_mQCboA8ijWK","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","#implementation of the loss function net here\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","  loss = tf.reduce_mean(xentropy, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GL_cXX09ih12","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"train\"):\n","  #implementation of the training optimizer here\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EfKWL5IZigJg","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","  correct = tf.nn.in_top_k(logits, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SKcVSGXOAGGT","colab_type":"text"},"cell_type":"markdown","source":["- - -\n","## 3. EXECUTION PHASE (30 pts)\n","\n","After you construct the three models of neural networks, you can compute the performance measure as the class accuracy. You will need to define the number of epochs and size of the training batch. You also might need to reset the graph each time your try a different model. To save time and avoid retraining, you should save the trained model and load it from disk to evaluate a test set. Pick the best model and answer the following:\n","- Which model yields the best performance measure for your dataset? Provide a reason why it yields the best performance.\n","- Why did you pick this many hidden layers?\n","- Provide some justifiable reasons for selecting the number of neurons per hidden layers. \n","- Which activation functions did you use?\n","\n","In the next session you will get a chance to finetune it further .\n","\n"]},{"metadata":{"id":"qV8wCeqyn3cC","colab_type":"text"},"cell_type":"markdown","source":["## Comment: I reorder the codes for each dnn I construct."]},{"metadata":{"id":"NGDKdeZzAGGV","colab_type":"code","colab":{}},"cell_type":"code","source":["# Your code goes here\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","n_epochs =70 \n","batch_size = 50\n","\n","# shuffle_batch() shuffle the examples in a batch before training\n","def shuffle_batch(X, y, batch_size):\n","    rnd_idx = np.random.permutation(len(X))\n","    n_batches = len(X) // batch_size\n","    for batch_idx in np.array_split(rnd_idx, n_batches):\n","        X_batch, y_batch = X[batch_idx], y[batch_idx]\n","        yield X_batch, y_batch\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MKMqf1gijPwW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"92c35961-f87a-4901-e5c0-3cb4779cfa7b","executionInfo":{"status":"ok","timestamp":1555601811328,"user_tz":240,"elapsed":262882,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","  print(\"results for DNN1\")\n","  init.run()\n","  for epoch in range(n_epochs):\n","    # implementation of the training ops here\n","    # implementation of the validation accuracy here\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","    if epoch % 5 == 0:\n","        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","    \n","  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["results for DNN1\n","0 Batch accuracy: 0.8 Validation accuracy: 0.8008\n","5 Batch accuracy: 0.86 Validation accuracy: 0.8547\n","10 Batch accuracy: 0.94 Validation accuracy: 0.8691\n","15 Batch accuracy: 0.86 Validation accuracy: 0.8756\n","20 Batch accuracy: 0.9 Validation accuracy: 0.8806\n","25 Batch accuracy: 0.96 Validation accuracy: 0.8788\n","30 Batch accuracy: 0.94 Validation accuracy: 0.8834\n","35 Batch accuracy: 0.9 Validation accuracy: 0.8856\n","40 Batch accuracy: 0.92 Validation accuracy: 0.8885\n","45 Batch accuracy: 0.96 Validation accuracy: 0.8881\n","50 Batch accuracy: 0.98 Validation accuracy: 0.8883\n","55 Batch accuracy: 0.92 Validation accuracy: 0.8911\n","60 Batch accuracy: 0.98 Validation accuracy: 0.8887\n","65 Batch accuracy: 0.96 Validation accuracy: 0.882\n"],"name":"stdout"}]},{"metadata":{"id":"lFPM62nDjncP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"9af70d56-97e5-402b-8a4b-4671727c06fe","executionInfo":{"status":"ok","timestamp":1555601812361,"user_tz":240,"elapsed":263898,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","    saver.restore(sess, \"./my_dnn_model.ckpt\")\n","    # implementation of the test set evaluation here\n","    for epoch in range(n_epochs):\n","      if epoch % 10 == 0:\n","          acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","          print(epoch, \"Test accuracy:\", acc_test)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0 Test accuracy: 0.88\n","10 Test accuracy: 0.88\n","20 Test accuracy: 0.88\n","30 Test accuracy: 0.88\n","40 Test accuracy: 0.88\n","50 Test accuracy: 0.88\n","60 Test accuracy: 0.88\n"],"name":"stdout"}]},{"metadata":{"id":"snnavcm0urN8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"861e6da4-e028-4cab-fbba-364e346c71ad","executionInfo":{"status":"ok","timestamp":1555601812362,"user_tz":240,"elapsed":263883,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["# print out the final accuracy here\n"," print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Final test accuracy: 88.00%\n"],"name":"stdout"}]},{"metadata":{"id":"L-bwmNWIbh4n","colab_type":"text"},"cell_type":"markdown","source":["## Dnn2"]},{"metadata":{"id":"luyNllyVkNxr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"c0c60322-3cee-40ab-cbdb-f62aa3a5c557","executionInfo":{"status":"ok","timestamp":1555602087059,"user_tz":240,"elapsed":538564,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["\n","reset_graph()\n","\n","# Set some configuration here\n","n_inputs = 28*28  # Fashion-MNIST\n","learning_rate = 0.01\n","n_outputs = 10\n","\n","\n","# Construct placeholder for the input layer\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","#add n_outputs by myself\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","\n","with tf.name_scope(\"dnn2\"):\n","  #implementation of the second net here\n","#     dnn2_hidden1 = 500\n","#     dnn2_hidden2 = 250\n","#     dnn2_hidden3 = 130\n","#     dnn2_hidden4 = 70\n","    dnn2_hidden1 = 400\n","    dnn2_hidden2 = 200\n","    dnn2_hidden3 = 100\n","    dnn2_hidden4 = 50\n","    dnn2_outputs=10\n","    hidden1_2 = tf.layers.dense(X, dnn2_hidden1, activation=tf.nn.elu, name=\"hidden1_2\")\n","    hidden2_2 = tf.layers.dense(hidden1_2, dnn2_hidden2, activation=tf.nn.elu, name=\"hidden2_2\")\n","    hidden3_2 = tf.layers.dense(hidden2_2, dnn2_hidden3, activation=tf.nn.elu, name=\"hidden3_2\")\n","    hidden4_2 = tf.layers.dense(hidden3_2, dnn2_hidden4, activation=tf.nn.elu, name=\"hidden4_2\")\n","    logits2 = tf.layers.dense(hidden4_2, dnn2_outputs, name=\"dnn2_outputs\")\n","    \n","\n","with tf.name_scope(\"loss\"):\n","#implementation of the loss function net here\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits2)\n","  loss = tf.reduce_mean(xentropy, name=\"loss\")\n","  \n","with tf.name_scope(\"train\"):\n","  #implementation of the training optimizer here\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","  correct = tf.nn.in_top_k(logits2, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  \n","  \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","n_epochs =70 \n","batch_size = 50\n","\n","# shuffle_batch() shuffle the examples in a batch before training\n","def shuffle_batch(X, y, batch_size):\n","    rnd_idx = np.random.permutation(len(X))\n","    n_batches = len(X) // batch_size\n","    for batch_idx in np.array_split(rnd_idx, n_batches):\n","        X_batch, y_batch = X[batch_idx], y[batch_idx]\n","        yield X_batch, y_batch\n","        \n","with tf.Session() as sess:\n","  print(\"results for DNN2\")\n","  init.run()\n","  for epoch in range(n_epochs):\n","    # implementation of the training ops here\n","    # implementation of the validation accuracy here\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","    if epoch % 5 == 0:\n","        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","    \n","  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")\n","  \n","with tf.Session() as sess:\n","    saver.restore(sess, \"./my_dnn_model.ckpt\")\n","    # implementation of the test set evaluation here\n","    for epoch in range(n_epochs):\n","      if epoch % 10 == 0:\n","          acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","          print(epoch, \"Test accuracy:\", acc_test)\n","          \n","          \n","# print out the final accuracy here\n","print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["results for DNN2\n","0 Batch accuracy: 0.84 Validation accuracy: 0.8121\n","5 Batch accuracy: 0.88 Validation accuracy: 0.8545\n","10 Batch accuracy: 0.94 Validation accuracy: 0.8682\n","15 Batch accuracy: 0.9 Validation accuracy: 0.8784\n","20 Batch accuracy: 0.9 Validation accuracy: 0.8811\n","25 Batch accuracy: 0.98 Validation accuracy: 0.8797\n","30 Batch accuracy: 0.94 Validation accuracy: 0.8859\n","35 Batch accuracy: 0.88 Validation accuracy: 0.8858\n","40 Batch accuracy: 0.86 Validation accuracy: 0.8864\n","45 Batch accuracy: 0.98 Validation accuracy: 0.8881\n","50 Batch accuracy: 0.96 Validation accuracy: 0.8887\n","55 Batch accuracy: 0.9 Validation accuracy: 0.8882\n","60 Batch accuracy: 1.0 Validation accuracy: 0.889\n","65 Batch accuracy: 0.96 Validation accuracy: 0.8819\n","0 Test accuracy: 0.8832\n","10 Test accuracy: 0.8832\n","20 Test accuracy: 0.8832\n","30 Test accuracy: 0.8832\n","40 Test accuracy: 0.8832\n","50 Test accuracy: 0.8832\n","60 Test accuracy: 0.8832\n","Final test accuracy: 88.32%\n"],"name":"stdout"}]},{"metadata":{"id":"WllffSvtoGtR","colab_type":"text"},"cell_type":"markdown","source":["## DNN 3"]},{"metadata":{"id":"JuvjZeJekP4-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"0de237dd-78ba-4772-dceb-86805d03b704","executionInfo":{"status":"ok","timestamp":1555602353532,"user_tz":240,"elapsed":805020,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["reset_graph()\n","\n","# Set some configuration here\n","n_inputs = 28*28  # Fashion-MNIST\n","learning_rate = 0.01\n","\n","\n","# Construct placeholder for the input layer\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","#add n_outputs by myself\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","\n","with tf.name_scope(\"dnn3\"):\n","  #implementation of the second net here\n","#     dnn3_hidden1 = 500\n","#     dnn3_hidden2 = 250\n","#     dnn3_hidden3 = 130\n","#     dnn3_outputs=10\n","    dnn3_hidden1 = 400\n","    dnn3_hidden2 = 200\n","    dnn3_hidden3 = 100\n","    dnn3_outputs=10\n","    hidden1_3 = tf.layers.dense(X, dnn3_hidden1, activation=tf.nn.elu, name=\"hidden1_3\")\n","    hidden2_3 = tf.layers.dense(hidden1_3, dnn3_hidden2, activation=tf.nn.elu, name=\"hidden2_3\")\n","    hidden3_3 = tf.layers.dense(hidden2_3, dnn3_hidden3, activation=tf.nn.elu, name=\"hidden3_3\")\n","    logits3 = tf.layers.dense(hidden3_3, dnn3_outputs, name=\"dnn3_outputs\")\n","    \n","\n","with tf.name_scope(\"loss\"):\n","#implementation of the loss function net here\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits3)\n","  loss = tf.reduce_mean(xentropy, name=\"loss\")\n","  \n","with tf.name_scope(\"train\"):\n","  #implementation of the training optimizer here\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","  correct = tf.nn.in_top_k(logits3, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  \n","  \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","n_epochs =70 \n","batch_size = 50\n","\n","# shuffle_batch() shuffle the examples in a batch before training\n","def shuffle_batch(X, y, batch_size):\n","    rnd_idx = np.random.permutation(len(X))\n","    n_batches = len(X) // batch_size\n","    for batch_idx in np.array_split(rnd_idx, n_batches):\n","        X_batch, y_batch = X[batch_idx], y[batch_idx]\n","        yield X_batch, y_batch\n","        \n","with tf.Session() as sess:\n","  print(\"results for DNN3\")\n","  init.run()\n","  for epoch in range(n_epochs):\n","    # implementation of the training ops here\n","    # implementation of the validation accuracy here\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","    if epoch % 5 == 0:\n","        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","    \n","  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")\n","  \n","with tf.Session() as sess:\n","    saver.restore(sess, \"./my_dnn_model.ckpt\")\n","    # implementation of the test set evaluation here\n","    for epoch in range(n_epochs):\n","      if epoch % 10 == 0:\n","          acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","          print(epoch, \"Test accuracy:\", acc_test)\n","          \n","          \n","# print out the final accuracy here\n","print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["results for DNN3\n","0 Batch accuracy: 0.8 Validation accuracy: 0.8162\n","5 Batch accuracy: 0.86 Validation accuracy: 0.8521\n","10 Batch accuracy: 0.92 Validation accuracy: 0.866\n","15 Batch accuracy: 0.86 Validation accuracy: 0.8752\n","20 Batch accuracy: 0.88 Validation accuracy: 0.8789\n","25 Batch accuracy: 0.96 Validation accuracy: 0.8765\n","30 Batch accuracy: 0.92 Validation accuracy: 0.8821\n","35 Batch accuracy: 0.88 Validation accuracy: 0.8834\n","40 Batch accuracy: 0.9 Validation accuracy: 0.8853\n","45 Batch accuracy: 0.96 Validation accuracy: 0.8897\n","50 Batch accuracy: 0.98 Validation accuracy: 0.8875\n","55 Batch accuracy: 0.9 Validation accuracy: 0.891\n","60 Batch accuracy: 0.96 Validation accuracy: 0.8876\n","65 Batch accuracy: 0.96 Validation accuracy: 0.8797\n","0 Test accuracy: 0.8814\n","10 Test accuracy: 0.8814\n","20 Test accuracy: 0.8814\n","30 Test accuracy: 0.8814\n","40 Test accuracy: 0.8814\n","50 Test accuracy: 0.8814\n","60 Test accuracy: 0.8814\n","Final test accuracy: 88.14%\n"],"name":"stdout"}]},{"metadata":{"id":"Y1GTrQkQoJth","colab_type":"text"},"cell_type":"markdown","source":["## DNN 4"]},{"metadata":{"id":"_phucBMV-QB-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"da7728ab-c281-4c27-84ec-71d8beced749","executionInfo":{"status":"ok","timestamp":1555602629994,"user_tz":240,"elapsed":1081471,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["reset_graph()\n","\n","# Set some configuration here\n","n_inputs = 28*28  # Fashion-MNIST\n","learning_rate = 0.01\n","\n","\n","# Construct placeholder for the input layer\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","#add n_outputs by myself\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","\n","with tf.name_scope(\"dnn4\"):\n","  #implementation of the second net here\n","#     dnn4_hidden1 = 500\n","#     dnn4_hidden2 = 250\n","#     dnn4_hidden3 = 130\n","#     dnn4_hidden4 = 70\n","#     dnn4_hidden5 = 40\n","    dnn4_hidden1 = 400\n","    dnn4_hidden2 = 1\n","    dnn4_hidden3 = 100\n","    dnn4_hidden4 = 50\n","    dnn4_hidden5 = 20\n","    dnn4_outputs=10\n","    hidden1_4 = tf.layers.dense(X, dnn4_hidden1, activation=tf.nn.elu, name=\"hidden1_4\")\n","    hidden2_4 = tf.layers.dense(hidden1_4, dnn4_hidden2, activation=tf.nn.elu, name=\"hidden2_4\")\n","    hidden3_4 = tf.layers.dense(hidden2_4, dnn4_hidden3, activation=tf.nn.elu, name=\"hidden3_4\")\n","    hidden4_4 = tf.layers.dense(hidden3_4, dnn4_hidden4, activation=tf.nn.elu, name=\"hidden4_4\")\n","    hidden5_4 = tf.layers.dense(hidden4_4, dnn4_hidden5, activation=tf.nn.elu, name=\"hidden5_4\")\n","                                 \n","    logits4 = tf.layers.dense(hidden5_4, dnn4_outputs, name=\"dnn4_outputs\")\n","    \n","\n","with tf.name_scope(\"loss\"):\n","#implementation of the loss function net here\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits4)\n","  loss = tf.reduce_mean(xentropy, name=\"loss\")\n","  \n","with tf.name_scope(\"train\"):\n","  #implementation of the training optimizer here\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","  correct = tf.nn.in_top_k(logits4, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  \n","  \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","n_epochs =70 \n","batch_size = 50\n","\n","# shuffle_batch() shuffle the examples in a batch before training\n","def shuffle_batch(X, y, batch_size):\n","    rnd_idx = np.random.permutation(len(X))\n","    n_batches = len(X) // batch_size\n","    for batch_idx in np.array_split(rnd_idx, n_batches):\n","        X_batch, y_batch = X[batch_idx], y[batch_idx]\n","        yield X_batch, y_batch\n","        \n","with tf.Session() as sess:\n","  print(\"results for DNN4\")\n","  init.run()\n","  for epoch in range(n_epochs):\n","    # implementation of the training ops here\n","    # implementation of the validation accuracy here\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","    if epoch % 5 == 0:\n","        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","    \n","  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")\n","  \n","with tf.Session() as sess:\n","    saver.restore(sess, \"./my_dnn_model.ckpt\")\n","    # implementation of the test set evaluation here\n","    for epoch in range(n_epochs):\n","      if epoch % 10 == 0:\n","          acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","          print(epoch, \"Test accuracy:\", acc_test)\n","          \n","          \n","# print out the final accuracy here\n","print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["results for DNN4\n","0 Batch accuracy: 0.84 Validation accuracy: 0.7944\n","5 Batch accuracy: 0.88 Validation accuracy: 0.857\n","10 Batch accuracy: 0.94 Validation accuracy: 0.8713\n","15 Batch accuracy: 0.88 Validation accuracy: 0.8827\n","20 Batch accuracy: 0.94 Validation accuracy: 0.8851\n","25 Batch accuracy: 0.96 Validation accuracy: 0.8822\n","30 Batch accuracy: 0.94 Validation accuracy: 0.8824\n","35 Batch accuracy: 0.92 Validation accuracy: 0.8882\n","40 Batch accuracy: 0.9 Validation accuracy: 0.887\n","45 Batch accuracy: 1.0 Validation accuracy: 0.8891\n","50 Batch accuracy: 1.0 Validation accuracy: 0.8866\n","55 Batch accuracy: 0.94 Validation accuracy: 0.8873\n","60 Batch accuracy: 0.98 Validation accuracy: 0.8875\n","65 Batch accuracy: 1.0 Validation accuracy: 0.8851\n","0 Test accuracy: 0.8793\n","10 Test accuracy: 0.8793\n","20 Test accuracy: 0.8793\n","30 Test accuracy: 0.8793\n","40 Test accuracy: 0.8793\n","50 Test accuracy: 0.8793\n","60 Test accuracy: 0.8793\n","Final test accuracy: 87.93%\n"],"name":"stdout"}]},{"metadata":{"id":"yivybbikxZ3U","colab_type":"text"},"cell_type":"markdown","source":["My second model with dnn2 performs best since it yields the highest accuracy of 88.32% of the three models I construct. I pick four hidden layers since I have tried two, three, four and five hidden layers. The accuracy rate increases as I add hidden layers from two to four, but it decreases as I add the fifth layer. I decrease the number of neurons by approximately 50%  for each additional layers. For instance, I use 400 neurons for my first layer, 200 neurons for my second layer, 100 for the third and 50 for the fourth. I use elu function for this model. "]},{"metadata":{"id":"-s2zv1SrAGGY","colab_type":"text"},"cell_type":"markdown","source":["- - -\n","## 4. FINETUNING THE NETWORK (25 pts)\n","\n","The best performance on the Fashion MNIST of a non-neural-net classifier is the Support Vector Classifier {\"C\":10,\"kernel\":\"poly\"} with 0.897 accuracy. In this section, you will see how close you can get to that accuracy, or (better yet) beat it! You will be able to see the performance of other ML methods below:\n","http://fashion-mnist.s3-website.eu-central-1.amazonaws.com\n","\n","Use the best model from the previous section and see if you can improve it further. To improve the performance of your model, You must make some modifications based upon the practical guidelines discuss in class. Here are a few decisions about the recommended network configurations you have to make:\n","1. Initialization: Use He Initialization for your model\n","2. Activation: Add ELU as the activation function throughout your hidden layers\n","3. Normalization: Incorporate the batch normalization at every layer\n","4. Regularization: Configure the dropout policy at 50% rate\n","5. Optimization: Change Gradient Descent into Adam Optimization\n","6. Your choice: make any other changes in 1-5 you deem necessary\n","\n","Keep in mind that the execution phase is essentially the same, so you can just run it from the above. See how much you gain in classification accuracy. Provide some justifications for the gain in performance. \n","\n","\n","\n","\n"]},{"metadata":{"id":"kkexa3xFdruX","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","# Set some configuration here\n","n_inputs = 28*28  # Fashion-MNIST\n","learning_rate = 0.01\n","\n","batch_norm_momentum = 0.9\n","training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","# Construct placeholder for the input layer\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","#add n_outputs by myself\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hjDXZ5ws6Zpx","colab_type":"code","colab":{}},"cell_type":"code","source":["dropout_rate = 0.5  # == 1 - keep_prob\n","X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n","\n","with tf.name_scope(\"dnnBenchmark\"):\n","   # implementation of the new benchmarking DNN here\n","    #He Initialization \n","    he_init = tf.variance_scaling_initializer()\n","    \n","#     dnnB_hidden1 = 500\n","#     dnnB_hidden2 = 250\n","#     dnnB_hidden3 = 130\n","#     dnnB_hidden4 = 70\n","    dnnB_hidden1 = 400\n","    dnnB_hidden2 = 200\n","    dnnB_hidden3 = 100\n","    dnnB_hidden4 = 50\n","    dnnB_outputs=10\n","    #Batch normalization && dropout\n","    training = tf.placeholder_with_default(False, shape=(), name='training')\n","    hidden1_B = tf.layers.dense(X_drop, dnnB_hidden1, name=\"hidden1_B\")\n","    bn1 = tf.layers.batch_normalization(hidden1_B, training=training, momentum=batch_norm_momentum)\n","    bn1_act = tf.nn.elu(bn1)\n","    bn1_dp = tf.layers.dropout(bn1_act, dropout_rate, training=training)\n","    hidden2_B = tf.layers.dense(bn1_dp, dnnB_hidden2, name=\"hidden2_B\")\n","    bn2 = tf.layers.batch_normalization(hidden2_B, training=training, momentum=batch_norm_momentum)\n","    bn2_act = tf.nn.elu(bn2)\n","    bn2_dp = tf.layers.dropout(bn2_act, dropout_rate, training=training)\n","   \n","  \n","    hidden3_B = tf.layers.dense(bn2_dp, dnnB_hidden3, name=\"hidden3_B\")\n","    bn3 = tf.layers.batch_normalization(hidden3_B, training=training, momentum=batch_norm_momentum)\n","    bn3_act = tf.nn.elu(bn3)\n","    bn3_dp = tf.layers.dropout(bn3_act, dropout_rate, training=training)\n","    \n","    \n","    hidden4_B = tf.layers.dense(bn3_dp, dnnB_hidden4, name=\"hidden4_B\")\n","    bn4 = tf.layers.batch_normalization(hidden2_B, training=training, momentum=batch_norm_momentum)\n","    bn4_act = tf.nn.elu(bn4)\n","    bn4_dp = tf.layers.dropout(bn4_act, dropout_rate, training=training)\n","  \n","    logits_before_bn = tf.layers.dense(bn4_dp, dnnB_outputs, name=\"dnnB_outputs\") \n","#     logits_before_bn = tf.layers.dense(bn2_dp, dnnB_outputs, name=\"dnnB_outputs\")\n","    logitsB = tf.layers.batch_normalization(logits_before_bn, training=training,\n","                                       momentum=0.9)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ELLMKFCdd0e4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"0deecd71-7bce-4cc3-ad74-849cef3beb45","executionInfo":{"status":"ok","timestamp":1555618897031,"user_tz":240,"elapsed":198455,"user":{"displayName":"Yuxin Wu","photoUrl":"","userId":"09590004171465787375"}}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","#implementation of the loss function net here\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logitsB)\n","  loss = tf.reduce_mean(xentropy, name=\"loss\")\n","  \n","with tf.name_scope(\"train\"):\n","  #implementation of the training optimizer here\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","  correct = tf.nn.in_top_k(logitsB, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  \n","  \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","n_epochs =50\n","batch_size =70\n","\n","# shuffle_batch() shuffle the examples in a batch before training\n","def shuffle_batch(X, y, batch_size):\n","    rnd_idx = np.random.permutation(len(X))\n","    n_batches = len(X) // batch_size\n","    for batch_idx in np.array_split(rnd_idx, n_batches):\n","        X_batch, y_batch = X[batch_idx], y[batch_idx]\n","        yield X_batch, y_batch\n","        \n","with tf.Session() as sess:\n","  print(\"results for Dnn Benchmark\")\n","  init.run()\n","  for epoch in range(n_epochs):\n","    # implementation of the validation accuracy here\n","    for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","    if epoch % 5 == 0:\n","        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","    \n","  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")\n","  \n","with tf.Session() as sess:\n","    saver.restore(sess, \"./my_dnn_model.ckpt\")\n","    # implementation of the test set evaluation here\n","    for epoch in range(n_epochs):\n","      if epoch % 10 == 0:\n","          acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","          print(epoch, \"Test accuracy:\", acc_test)\n","          \n","          \n","# print out the final accuracy here\n","print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":94,"outputs":[{"output_type":"stream","text":["results for Dnn Benchmark\n","0 Batch accuracy: 0.8142857 Validation accuracy: 0.83\n","5 Batch accuracy: 0.87142855 Validation accuracy: 0.8738\n","10 Batch accuracy: 0.9714286 Validation accuracy: 0.8835\n","15 Batch accuracy: 0.9 Validation accuracy: 0.8849\n","20 Batch accuracy: 0.95714283 Validation accuracy: 0.8893\n","25 Batch accuracy: 0.98571426 Validation accuracy: 0.8847\n","30 Batch accuracy: 0.94285715 Validation accuracy: 0.8913\n","35 Batch accuracy: 0.9285714 Validation accuracy: 0.8872\n","40 Batch accuracy: 0.8857143 Validation accuracy: 0.8939\n","45 Batch accuracy: 1.0 Validation accuracy: 0.8891\n","0 Test accuracy: 0.8899\n","10 Test accuracy: 0.8899\n","20 Test accuracy: 0.8899\n","30 Test accuracy: 0.8899\n","40 Test accuracy: 0.8899\n","Final test accuracy: 88.99%\n"],"name":"stdout"}]},{"metadata":{"id":"sx2TXlgD_4t-","colab_type":"text"},"cell_type":"markdown","source":["how much you gain in classification accuracy. Provide some justifications for the gain in performance.\n","\n","My DNN benchmark model has an accuracy of 88.99%, which increases accuracy by 0.67% compared to the best model I got from the last section, the dnn2 model with an accuracy of 88.32%. I think the reason why my performance becomes better is that I add He initialization, batch normalization and change Gradient Decent into Adma optimization. I also change my batch size from 50 to 70 and decrease my iterations from 70 to 50. "]},{"metadata":{"id":"04jsbI9TAGGY","colab_type":"text"},"cell_type":"markdown","source":["- - -\n","## 5. OUTLOOK (5 pts)\n","\n","Plan for the outlook of your system: This may lead to the direction of your future project:\n","- Did your neural network outperform other \"traditional ML technique? Why/why not?\n","- Does your model work well? If not, which model should be further investigated?\n","- Do you satisfy with your system? What do you think needed to improve?\n","\n"]},{"metadata":{"id":"yiO_MxWv6V13","colab_type":"text"},"cell_type":"markdown","source":["My neural network outperforms most of other traditional ML techniques since the accuracy rate of my model is higher than all other models except SVC. I think my model is good and can work better with more experiments. Due to the limited computing power, it takes me a long time to run a model, so I dont't have time to try many different parameters. I think I would do more research on the rules to decided the number of nodes of each hidden layers and try more combinations. I will have a better and more specific learning schedule so that the results can be more closed the optimal points. Also, I may try different dropout rate or momentum to see whether it will improve the performance. Furthermore, I'd like to investigate convolutional neural network models."]},{"metadata":{"id":"zS9PKaL4AGGZ","colab_type":"text"},"cell_type":"markdown","source":["- - - \n","### NEED HELP?"]},{"metadata":{"id":"T0vuIEBDAGGa","colab_type":"text"},"cell_type":"markdown","source":["In case you get stuck in any step in the process, you may find some useful information from:\n","\n"," * Consult my lectures and/or the textbook\n"," * Talk to the TA, they are available and there to help you during OH\n"," * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4501 Assignment 4:...\".\n"," * More on the Fashion-MNIST to be found here: https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/\n","\n","Best of luck and have fun!"]},{"metadata":{"id":"cH_mulWEAGGb","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}